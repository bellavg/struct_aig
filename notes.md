Base Model: Structure-Aware Transformer (SAT)

Attention: Replace SAT's standard self-attention with the paper's DAGRA.

Positional Encoding: Replace SAT's original PE with the paper's DAGPE.